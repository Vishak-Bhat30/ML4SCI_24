{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783b8a45",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-25T13:43:23.515607Z",
     "iopub.status.busy": "2024-03-25T13:43:23.515100Z",
     "iopub.status.idle": "2024-03-25T13:43:34.385582Z",
     "shell.execute_reply": "2024-03-25T13:43:34.384302Z"
    },
    "papermill": {
     "duration": 10.881445,
     "end_time": "2024-03-25T13:43:34.388552",
     "exception": false,
     "start_time": "2024-03-25T13:43:23.507107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix ,roc_curve\n",
    "import seaborn as sns\n",
    "from skimage.segmentation import slic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edef1178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T13:43:34.402084Z",
     "iopub.status.busy": "2024-03-25T13:43:34.401497Z",
     "iopub.status.idle": "2024-03-25T13:44:07.298625Z",
     "shell.execute_reply": "2024-03-25T13:44:07.297351Z"
    },
    "papermill": {
     "duration": 32.906818,
     "end_time": "2024-03-25T13:44:07.301329",
     "exception": false,
     "start_time": "2024-03-25T13:43:34.394511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\r\n",
      "  Downloading torch_geometric-2.5.2-py3-none-any.whl.metadata (64 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.11.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2024.3.0)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.9.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.31.0)\r\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.2.2)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (4.0.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2024.2.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (3.2.0)\r\n",
      "Downloading torch_geometric-2.5.2-py3-none-any.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: torch_geometric\r\n",
      "Successfully installed torch_geometric-2.5.2\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (3.2.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3037028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T13:44:07.317661Z",
     "iopub.status.busy": "2024-03-25T13:44:07.317178Z",
     "iopub.status.idle": "2024-03-25T13:44:11.113957Z",
     "shell.execute_reply": "2024-03-25T13:44:11.112719Z"
    },
    "papermill": {
     "duration": 3.808361,
     "end_time": "2024-03-25T13:44:11.116858",
     "exception": false,
     "start_time": "2024-03-25T13:44:07.308497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.nn import NNConv, global_mean_pool\n",
    "from torch_geometric.nn import GATConv\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b7ce4c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T13:44:11.133367Z",
     "iopub.status.busy": "2024-03-25T13:44:11.131984Z",
     "iopub.status.idle": "2024-03-25T13:44:28.712671Z",
     "shell.execute_reply": "2024-03-25T13:44:28.711463Z"
    },
    "papermill": {
     "duration": 17.592062,
     "end_time": "2024-03-25T13:44:28.715778",
     "exception": false,
     "start_time": "2024-03-25T13:44:11.123716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 25\n",
    "\n",
    "# List of Parquet file paths\n",
    "file_paths = [\n",
    "    '/kaggle/input/task2-24/QCDToGGQQ_IMGjet_RH1all_jet0_run0_n36272.test.snappy.parquet',\n",
    "    '/kaggle/input/task2-24/QCDToGGQQ_IMGjet_RH1all_jet0_run1_n47540.test.snappy.parquet',\n",
    "    '/kaggle/input/task2-24/QCDToGGQQ_IMGjet_RH1all_jet0_run2_n55494.test.snappy.parquet'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop through each file path\n",
    "for file_path in file_paths:\n",
    "    # Create a Parquet file reader object\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    \n",
    "    # Determine the total number of rows in the file\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    \n",
    "    # Calculate the number of chunks\n",
    "    num_chunks = total_rows // chunk_size + (1 if total_rows % chunk_size else 0)\n",
    "    \n",
    "    # Loop over the file in chunks\n",
    "    for chunk_index in range(num_chunks):\n",
    "        # Read a chunk of rows from the file\n",
    "        chunk = parquet_file.read_row_group(chunk_index, columns=None)\n",
    "        df = chunk.to_pandas()\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames into a single DataFrame\n",
    "data = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb69d8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T13:44:28.732198Z",
     "iopub.status.busy": "2024-03-25T13:44:28.731791Z",
     "iopub.status.idle": "2024-03-25T13:44:28.738846Z",
     "shell.execute_reply": "2024-03-25T13:44:28.737685Z"
    },
    "papermill": {
     "duration": 0.01782,
     "end_time": "2024-03-25T13:44:28.741252",
     "exception": false,
     "start_time": "2024-03-25T13:44:28.723432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_3d(arr):\n",
    "    vishak=[]\n",
    "    for i in range (0,3):\n",
    "        vis=np.stack(np.stack(arr)[i],axis=-1)\n",
    "        vishak.append(vis)\n",
    "    vishak=np.array(vishak)\n",
    "    vishak_max = vishak.max()\n",
    "    vishak_min = vishak.min()\n",
    "    vishak = (vishak - vishak_min)/(vishak_max - vishak_min)\n",
    "    return vishak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c23fcb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T13:44:28.757636Z",
     "iopub.status.busy": "2024-03-25T13:44:28.756345Z",
     "iopub.status.idle": "2024-03-25T13:44:35.664032Z",
     "shell.execute_reply": "2024-03-25T13:44:35.662979Z"
    },
    "papermill": {
     "duration": 6.918566,
     "end_time": "2024-03-25T13:44:35.666786",
     "exception": false,
     "start_time": "2024-03-25T13:44:28.748220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[\"X_jets\"]  = data[\"X_jets\"].apply(to_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a1d736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T13:44:35.685315Z",
     "iopub.status.busy": "2024-03-25T13:44:35.684125Z",
     "iopub.status.idle": "2024-03-25T13:44:35.696203Z",
     "shell.execute_reply": "2024-03-25T13:44:35.694901Z"
    },
    "papermill": {
     "duration": 0.024024,
     "end_time": "2024-03-25T13:44:35.699147",
     "exception": false,
     "start_time": "2024-03-25T13:44:35.675123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_to_graph(image, patch_size=25, n_neighbors=15):\n",
    "        \"\"\"\n",
    "        Convert an image to a graph of its 5x5 patches.\n",
    "\n",
    "        Parameters:\n",
    "        - image: A (125, 125, 3) numpy array.\n",
    "        - patch_size: Size of the square patches (default 5).\n",
    "        - n_neighbors: Number of neighbors for KNN (default 5).\n",
    "\n",
    "        Returns:\n",
    "        - nodes: An array of node features.\n",
    "        - edges: A list of tuples (i, j, mse) representing edges and their MSE.\n",
    "        \"\"\"\n",
    "        # Validate image shape\n",
    "        \n",
    "        assert image.shape[0] == image.shape[1], \"Image must be square.\"\n",
    "\n",
    "        # Number of patches along one dimension\n",
    "        num_patches = image.shape[0] // patch_size\n",
    "\n",
    "        # Initialize nodes and edges\n",
    "        nodes = []\n",
    "        edges = []\n",
    "\n",
    "        # Create patches and flatten them to create node features\n",
    "        for i in range(0, image.shape[0], patch_size):\n",
    "            for j in range(0, image.shape[1], patch_size):\n",
    "                patch = image[i:i+patch_size, j:j+patch_size, :].reshape(-1)\n",
    "                nodes.append(patch)\n",
    "\n",
    "        nodes = np.array(nodes)\n",
    "\n",
    "        # Use KNN to find nearest neighbors for each node\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors+1, algorithm='ball_tree').fit(nodes)\n",
    "        distances, indices = nbrs.kneighbors(nodes)\n",
    "\n",
    "        # Calculate MSE for each pair of neighbors and create edges\n",
    "        for i in range(indices.shape[0]):\n",
    "            for j in range(1, indices.shape[1]):  # Start from 1 to skip self-connection\n",
    "                mse = mean_squared_error(nodes[i], nodes[indices[i, j]])\n",
    "                edges.append((i, indices[i, j], mse))\n",
    "\n",
    "        return nodes, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d9889d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T13:44:35.715117Z",
     "iopub.status.busy": "2024-03-25T13:44:35.714655Z",
     "iopub.status.idle": "2024-03-25T13:44:35.727303Z",
     "shell.execute_reply": "2024-03-25T13:44:35.725845Z"
    },
    "papermill": {
     "duration": 0.024068,
     "end_time": "2024-03-25T13:44:35.730194",
     "exception": false,
     "start_time": "2024-03-25T13:44:35.706126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QuarkGluonDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, root='', transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset for quarks and gluons classification.\n",
    "        \n",
    "        Parameters:\n",
    "        - image_list: A list of (125, 125, 3) numpy arrays.\n",
    "        - labels: A list of integers (0 or 1) representing the class labels for the images.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        super(QuarkGluonDataset, self).__init__(root, transform, pre_transform)\n",
    "    \n",
    "   \n",
    "    def len(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        # Convert an image to graph data\n",
    "        image = self.dataframe.iloc[idx]['X_jets']\n",
    "        image = image.transpose(1,2,0)\n",
    "        label = self.dataframe.iloc[idx]['y']\n",
    "#         print(type(image))\n",
    "        nodes, edges = image_to_graph(image)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        x = torch.tensor(nodes, dtype=torch.float)  # Node features\n",
    "        edge_index = torch.tensor([(i, j) for i, j, _ in edges], dtype=torch.long).t().contiguous()  # Edge indices\n",
    "        edge_attr = torch.tensor([mse for _, _, mse in edges], dtype=torch.float).unsqueeze(1)  # Edge attributes\n",
    "        y = torch.tensor([label], dtype=torch.long)  # Label\n",
    "        \n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d877841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T13:44:35.745965Z",
     "iopub.status.busy": "2024-03-25T13:44:35.745528Z",
     "iopub.status.idle": "2024-03-25T13:44:35.775542Z",
     "shell.execute_reply": "2024-03-25T13:44:35.774176Z"
    },
    "papermill": {
     "duration": 0.041392,
     "end_time": "2024-03-25T13:44:35.778579",
     "exception": false,
     "start_time": "2024-03-25T13:44:35.737187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = QuarkGluonDataset(data)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# Perform the random split\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create the DataLoaders for the train and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9463d235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T13:44:35.794647Z",
     "iopub.status.busy": "2024-03-25T13:44:35.794163Z",
     "iopub.status.idle": "2024-03-25T13:44:35.967890Z",
     "shell.execute_reply": "2024-03-25T13:44:35.966631Z"
    },
    "papermill": {
     "duration": 0.184689,
     "end_time": "2024-03-25T13:44:35.970526",
     "exception": false,
     "start_time": "2024-03-25T13:44:35.785837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSAGENet(\n",
      "  (sage1): SAGEConv(1875, 512, aggr=mean)\n",
      "  (sage2): SAGEConv(512, 256, aggr=mean)\n",
      "  (sage3): SAGEConv(256, 128, aggr=mean)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (lin): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from torch.nn import Linear, Dropout\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphSAGENet(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GraphSAGENet, self).__init__()\n",
    "        self.sage1 = SAGEConv(num_node_features, 512)\n",
    "        self.sage2 = SAGEConv(512, 256)\n",
    "        self.sage3 = SAGEConv(256, 128)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.lin = Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = F.relu(self.sage1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.sage2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.sage3(x, edge_index))\n",
    "\n",
    "        x = global_mean_pool(x, batch)  # Pooling to use graph-level features\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize the GraphSAGE model\n",
    "num_node_features = 1875  # Adjust according to your dataset\n",
    "num_classes = 2  # Assuming binary classification\n",
    "\n",
    "model = GraphSAGENet(num_node_features=num_node_features, num_classes=num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dda158d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T13:44:35.986962Z",
     "iopub.status.busy": "2024-03-25T13:44:35.986526Z",
     "iopub.status.idle": "2024-03-25T16:00:14.085451Z",
     "shell.execute_reply": "2024-03-25T16:00:14.083974Z"
    },
    "papermill": {
     "duration": 8138.112097,
     "end_time": "2024-03-25T16:00:14.089946",
     "exception": false,
     "start_time": "2024-03-25T13:44:35.977849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:43<00:00,  2.88s/it, loss=0.588]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Best Model: Epoch 1, Val. Acc.: 0.6063\n",
      "Epoch: 001, Train Loss: 0.6677, Val. Loss: 0.6693, Val. Acc.: 0.6063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:43<00:00,  2.88s/it, loss=0.491]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Best Model: Epoch 2, Val. Acc.: 0.6117\n",
      "Epoch: 002, Train Loss: 0.6564, Val. Loss: 0.6668, Val. Acc.: 0.6117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:43<00:00,  2.88s/it, loss=0.591]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Train Loss: 0.6535, Val. Loss: 0.6678, Val. Acc.: 0.6009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:45<00:00,  2.90s/it, loss=0.456]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Best Model: Epoch 4, Val. Acc.: 0.6152\n",
      "Epoch: 004, Train Loss: 0.6518, Val. Loss: 0.6693, Val. Acc.: 0.6152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:46<00:00,  2.90s/it, loss=0.61]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Train Loss: 0.6518, Val. Loss: 0.6707, Val. Acc.: 0.5964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:45<00:00,  2.90s/it, loss=0.508]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Train Loss: 0.6407, Val. Loss: 0.6714, Val. Acc.: 0.5803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:51<00:00,  2.94s/it, loss=0.683]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Train Loss: 0.6360, Val. Loss: 0.6722, Val. Acc.: 0.5892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:47<00:00,  2.91s/it, loss=0.805]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Best Model: Epoch 8, Val. Acc.: 0.6233\n",
      "Epoch: 008, Train Loss: 0.6310, Val. Loss: 0.6810, Val. Acc.: 0.6233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:49<00:00,  2.92s/it, loss=0.801]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Train Loss: 0.6164, Val. Loss: 0.6860, Val. Acc.: 0.6036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:56<00:00,  2.97s/it, loss=0.667]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Train Loss: 0.6062, Val. Loss: 0.6926, Val. Acc.: 0.6170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:51<00:00,  2.94s/it, loss=0.585]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Train Loss: 0.6008, Val. Loss: 0.7020, Val. Acc.: 0.5982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:48<00:00,  2.92s/it, loss=0.922]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Train Loss: 0.5924, Val. Loss: 0.7067, Val. Acc.: 0.5892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:52<00:00,  2.95s/it, loss=0.719]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Train Loss: 0.5746, Val. Loss: 0.7608, Val. Acc.: 0.6117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:51<00:00,  2.94s/it, loss=0.463]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Train Loss: 0.5567, Val. Loss: 0.7508, Val. Acc.: 0.5964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:51<00:00,  2.94s/it, loss=0.525]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Train Loss: 0.5353, Val. Loss: 0.7901, Val. Acc.: 0.5830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 140/140 [06:47<00:00,  2.91s/it, loss=0.425]\n",
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Move the model to the chosen device\n",
    "model.to(device)\n",
    "\n",
    "# Modify the train function to include loss in tqdm\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    with tqdm(train_loader, desc=\"Training\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Ensure the model forward call matches the expected arguments\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "# Similar modifications for the test/validation function, including data transfer to the device\n",
    "def test(model, loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad(), tqdm(loader, desc=\"Evaluating\", leave=False) as tepoch:\n",
    "        for data in tepoch:\n",
    "            data = data.to(device)\n",
    "            # Match the model forward call with expected arguments\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += int((pred == data.y).sum())\n",
    "    return correct / len(loader.dataset), total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(1, 100):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "#     train_acc, _ = test(model, train_loader, criterion)\n",
    "    val_acc, val_loss = test(model, valid_loader, criterion)\n",
    "    if(val_loss - train_loss >0.3):\n",
    "        print(\"Early Stopping\")\n",
    "        break\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"Saved Best Model: Epoch {epoch}, Val. Acc.: {val_acc:.4f}\")\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val. Loss: {val_loss:.4f}, Val. Acc.: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683bfb8",
   "metadata": {
    "papermill": {
     "duration": 0.491913,
     "end_time": "2024-03-25T16:00:15.062538",
     "exception": false,
     "start_time": "2024-03-25T16:00:14.570625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e63ea2",
   "metadata": {
    "papermill": {
     "duration": 0.490468,
     "end_time": "2024-03-25T16:00:16.041350",
     "exception": false,
     "start_time": "2024-03-25T16:00:15.550882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a16f8ce",
   "metadata": {
    "papermill": {
     "duration": 0.488503,
     "end_time": "2024-03-25T16:00:17.113599",
     "exception": false,
     "start_time": "2024-03-25T16:00:16.625096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4628811,
     "sourceId": 7885350,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8220.364084,
   "end_time": "2024-03-25T16:00:20.543200",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-25T13:43:20.179116",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
